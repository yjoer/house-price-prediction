---
title: "Programming of Data Science"
author: "Group 3"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: false
---

| Name                 | Matrix Number |
|----------------------|---------------|
| Hoo Xing Yu          | S2117839      |
| Pei Lin Soh          | S2193368      |
| Low Kian Haw         | S2190839      |
| Clarice Lau Yeo Hang | S2192784      |
| Yeoh Joer            | 22077700      |

### Introduction

-   Due to intense competition, the housing market necessitates constant innovation and optimisation.
-   A machine learning approach has enormous promise to address these issues.
-   Housing development and sales processes can be improved by using artificial intelligence and data analysis to ensure efficiency, profitability, and responsiveness to changing client demands.
-   In particular, this study aims to investigate the relationship between house attributes and sales by applying machine learning approaches to optimise housing development and sales.
-   Finding the elements that appeal to customers the most is the aim.
-   Industry experts can take crucial lessons from this inquiry on utilising machine learning to succeed in the dynamic real estate market.

#### **Problem Statement:**

-   Clustering -- Group house tiers based on their features to identify common characteristics and attributes that can inform our decision-making process in terms of what type of houses we should build.

-   Prediction -- Predict house prices based on various features such as location, size, number of rooms, and other amenities. This information will allow us to price our properties competitively and attract potential buyers.

-   Association rule -- Determine customers\' preferences on renovation based on house types to inform our renovation plans and maximize the number of houses sold.

### Research Objective

-   Maximize revenue and customer satisfaction while minimizing costs and risk.

-   Identify best type of properties with competitive prices to attract potential buyers.

### Data Description/ Source

-   The dataset is collected from Kaggle and it contains test and train dataset.

-   Data Source: <https://www.kaggle.com/datasets/prevek18/ames-housing-dataset>

### Import Necessary Libraries

```{r}
suppressPackageStartupMessages({
  library(arules)
  library(caret)
  library(catboost)
  library(cluster)
  library(corrplot)
  library(dplyr)
  library(e1071)
  library(factoextra)
  library(ggplot2)
  library(glmnet)
  library(gridExtra)
  library(lattice)
  library(lightgbm)
  library(mlr)
  library(mlrMBO)
  library(psych)
  library(randomForest)
  library(readr)
  library(stats)
  library(tidyr)
  library(viridis)
  library(xgboost)
  library(DescTools)
  library(Metrics)
  library(Rtsne)
  library(DiceKriging)
})
```

### Import Dataset

-   Dataset contains information on housing features such as location, size, number of rooms, and other amenities.

```{r}
dataset <- read.csv("train.csv")
head(dataset)
```

### Dataset Overview

-   There are 1460 records (rows) and 81 attributes (columns).

-   Dataset are mostly made up of numerical value (int64 & float64).

```{r}
cat("Total rows & columns:", dim(dataset), '\n\n')
str(dataset)
```

### Data Pre-processing

#### Check for Sum of Duplicates Between Columns

-   No duplication inside the dataset.

```{r}
duplicates <- sum(duplicated(dataset))
cat(paste("Number of duplicates:", duplicates))
```

#### Remove Meaningless Columns

-   At this stage, we have removed irrelevant columns from the dataset.

-   'Id' column is removed, as it is a unique identifier for each customer.

-   'GarageCars' column is removed, as it provides redundant information already captured by the 'GarageArea' column.

-   We also removed the 'Exterior2nd' column, as it represents a limited set of additional choices for exterior covering.

-   Furthermore, we removed the 'GarageYrBlt' column, as the year of construction is typically the same as the 'YearBuilt' column.

-   Finally, we removed the 'TotRmsAbvGrd' column, as it provides a similar meaning as the 'First Floor' square footage.

```{r}
# Subset the dataset by removing specific columns
dataset <- subset(dataset, select = -c(Id, GarageCars, Exterior2nd, GarageYrBlt, TotRmsAbvGrd))
```

#### Check for Missing Values

-   There are a total of 19 columns with missing values

-   5 of them have missing values higher than 20%

-   Columns with missing values higher than 20%: PoolQC, MiscFeature, Alley, Fence, FireplaceQu

-   Column with missing values higher than 20% will be removed in this stage

```{r}
# Calculate missing percentage
missing_percentages <- colSums(is.na(dataset)) / nrow(dataset) * 100

# Create a data frame with column names and missing percentages
missing_dataset <- data.frame(
  column_name = names(missing_percentages), 
  missing_percentage = missing_percentages
) %>%
  # Sort the data frame by missing percentages in descending order
  arrange(desc(missing_percentage)) %>%
  # Filter out columns with 0 missing percentages
  filter(missing_percentage > 0)

# Print the missing dataset
print(missing_dataset)
```

#### Remove Columns With More Than 20% Missing Values

```{r}
dataset <- subset(dataset, select = -c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu))
```

#### Impute Missing Data

-   Median is chosen to fill in missing values, rather than the mean.

-   The mean is sensitive to extreme values and can be heavily influenced by outliers in the data, which can lead to biased estimates.

-   In contrast, the median is a robust measure of central tendency that is less sensitive to extreme values and is more appropriate for skewed data.

```{r}
# Identify numerical columns & replace missing numerical values with median
num_cols <- sapply(dataset, is.numeric)
for (col in names(dataset[, num_cols])) {
  col_median <- median(dataset[, col], na.rm = TRUE)
  dataset[, col][is.na(dataset[, col])] <- rep(col_median, sum(is.na(dataset[, col])))
}
```

```{r}
# Identify categorical columns & replace missing categorical values with mode
cat_cols <- sapply(dataset, is.factor) | sapply(dataset, is.character)
for (col in names(dataset[, cat_cols])) {
  dataset[, col][is.na(dataset[, col])] <- as.character(Mode(dataset[, col], na.rm = TRUE))
}
```

#### Data Transformation/Scaling

-   Data pre-processing steps such as log transformation, scaling, and one-hot encoding.

-   Data splitted into train, validation, and test sets and combines the training and validation sets for model training.

-   Calculates and displays the skewness of the numerical features and target variable before and after the log transformations.

```{r}
# Select numerical columns using the "is.numeric" function
numerical_cols <- names(dataset)[sapply(dataset, is.numeric)]
categorical_cols <- names(dataset)[sapply(dataset, is.factor) | sapply(dataset, is.character)]

# Create new data frames with only numerical and categorical columns
numerical_data <- dataset[numerical_cols]
categorical_data <- dataset[categorical_cols]
```

```{r}
# Split the numerical variables into features and the target variable.
X_num <- subset(numerical_data, select = -c(SalePrice))
y <- subset(numerical_data, select = c(SalePrice))$SalePrice
```

```{r}
# Log Transformation for numerical features.
skewness_before <- sapply(X_num, function(x) {
  e1071::skewness(x)
})

X_num_skewed <- skewness_before[abs(skewness_before) > 0.75]

for (x in names(X_num_skewed)) {
  X_num[[x]] <- log1p(X_num[[x]])
}

skewness_after <- sapply(X_num, function(x) {
  e1071::skewness(x)
})

data.frame(skewness_before, skewness_after)
```

```{r}
# Log Transformation for the target variable.
skewness_before <- e1071::skewness(y)

y_t <- log1p(y)

skewness_after <- e1071::skewness(y_t)

sprintf("Before: %f, After: %f", skewness_before, skewness_after)
```

```{r}
# Scaling
X_num <- scale(X_num)
```

```{r}
# One-Hot Encoding for categorical variables.
encoder <- dummyVars(~., data = categorical_data)

X_cat <- predict(encoder, newdata = categorical_data)
X_cat <- data.frame(X_cat)
```

```{r}
# Split into train and validation and test sets.
X <- cbind(X_cat, X_num)

set.seed(12345)
train_idx <- createDataPartition(y_t, p = 0.7, list = F)
X_train <- X[train_idx, ]
y_train <- y_t[train_idx]

X_test <- X[-train_idx, ]
y_test <- y_t[-train_idx]

train_val_idx <- createDataPartition(y_train, p = 0.8, list = FALSE)
X_train <- X_train[train_val_idx, ]
y_train <- y_train[train_val_idx]

X_val <- X_train[-train_val_idx, ]
y_val <- y_train[-train_val_idx]

X_train_val <- rbind(X_train, X_val)
y_train_val <- c(y_train, y_val)
```

```{r}
dim(X)
length(y)

dim(X_train)
length(y_train)

dim(X_val)
length(y_val)

#names(X)
```

### Explore Data - Exploratory Data Analysis (EDA)

#### Plot Distribution of Numeric Data

-   Majority of numerical columns' distribution are right-skewed (32 attributes), however, 2 of them are left-skewed

```{r}
# Display the first six rows of the numerical data frame
head(numerical_data)
```

```{r}
# Create a function to plot probability density plot and skewness
plot_density <- function(column, col_name) {
  # Compute the skewness of the data
  skewness <- skew(column)

  # Set the plot color based on the skewness
  if (skewness < 0) {
    plot_color <- "blue"
  } else {
    plot_color <- "red"
  }

  # Create the density plot
  plot_title <- col_name  # Rename the plot title with the column name
  density_plot <- ggplot(data = data.frame(column), aes(x = column)) +
    geom_density(fill = plot_color, alpha = 0.3) +
    ggtitle(plot_title)

  return(density_plot)
}
```

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Apply the plot_density function to each column of numerical data in numerical_data
# Select the first 20 column names from numerical_data
first_20 <- names(numerical_data)[1:20]

# Apply the plot_density function to each selected column
plots <- lapply(first_20, function(col_name) {
  plot_density(numerical_data[[col_name]], col_name)
})

# Arrange the resulting plots in a grid with 5 columns
grid.arrange(grobs = plots, ncol = 5)
```

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Select the first 20 column names from numerical_data
rest_14 <- names(numerical_data)[21:34]

# Apply the plot_density function to each selected column
plots <- lapply(rest_14, function(col_name) {
  plot_density(numerical_data[[col_name]], col_name)
})

# Arrange the resulting plots in a grid with 5
grid.arrange(grobs = plots, ncol = 5)
```

#### Check Outliers Using Boxplot

-   Outliers are found in most of the columns - 29 attributes

-   "MSSubClass", "LotFrontage", "LotArea", "OverallQual", "OverallCond", "YearBuilt", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "BedroomAbvGr", "KitchenAbvGr", "Fireplaces", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "SalePrice"

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Create a list to store the column names with outliers
columns_with_outliers <- c()

# Create a list of boxplots for the first 20 attributes
boxplot_numerical_first20attributes <- lapply(1:20, function(i) {
  # Calculate the lower and upper fences for outlier detection
  Q1 <- quantile(numerical_data[, i], 0.25)
  Q3 <- quantile(numerical_data[, i], 0.75)
  IQR <- Q3 - Q1
  lower_fence <- Q1 - 1.5 * IQR
  upper_fence <- Q3 + 1.5 * IQR

  # Check for outliers
  outliers <- numerical_data[, i] < lower_fence | numerical_data[, i] > upper_fence

  # Add the column name to the list if outliers are present
  if (any(outliers)) {
    columns_with_outliers <<- c(columns_with_outliers, colnames(numerical_data)[i])
  }

  # Create the boxplot
  ggplot(numerical_data, aes(x = 1, y = numerical_data[, i])) +
    geom_boxplot() +
    ggtitle(paste0("Boxplot of", colnames(numerical_data)[i])) +
    xlab("") +
    ylab(colnames(numerical_data)[i])
})

# Arrange the resulting boxplots in a grid with 5 columns
grid.arrange(grobs = boxplot_numerical_first20attributes, ncol = 5)
```

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Create a list of boxplots for the rest
boxplot_numerical_restattributes <- lapply(21:34, function(i) {
  # Calculate the lower and upper fences for outlier detection
  Q1 <- quantile(numerical_data[, i], 0.25)
  Q3 <- quantile(numerical_data[, i], 0.75)
  IQR <- Q3 - Q1
  lower_fence <- Q1 - 1.5 * IQR
  upper_fence <- Q3 + 1.5 * IQR

  # Check for outliers
  outliers <- numerical_data[, i] < lower_fence | numerical_data[, i] > upper_fence

  # Add the column name to the list if outliers are present
  if (any(outliers)) {
    columns_with_outliers <<- c(columns_with_outliers, colnames(numerical_data)[i])
  }

  # Create the boxplot
  ggplot(numerical_data, aes(x = 1, y = numerical_data[, i])) +
    geom_boxplot() +
    ggtitle(paste0("Boxplot of", colnames(numerical_data)[i])) +
    xlab("") +
    ylab(colnames(numerical_data)[i])
})

# Arrange the resulting boxplots in a grid with 5 columns
grid.arrange(grobs = boxplot_numerical_restattributes, ncol = 5)
```

```{r}
# Print the column names with outliers
print(columns_with_outliers)
```

#### Plot Correlation Matrix

-   Correlation with \>0.8 and above:

-   X1stFlrSF & TotalBsmtSF \|\| 0.81953

```{r, fig.width=10, fig.fullwidth=TRUE}
# Create a heat map of the correlation matrix
correlation_matrix <- cor(numerical_data, method = "pearson")
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", tl.col = "black", tl.cex = 0.3, tl.srt = 90)
```

```{r}
# Print correlation higher than 0.8
high_corr <- which(abs((correlation_matrix) > 0.8 | correlation_matrix < -0.8)& correlation_matrix!= 1, arr.ind=TRUE)
high_corr_values <- correlation_matrix[high_corr]
high_corr_df <- data.frame(row = high_corr[, 1], col = high_corr[, 2], corr = high_corr_values)
high_corr_df
```

#### Bar Plots for Categorical Data

-   There are total of 37 categorical data column

```{r}
# Display the first six rows of the categorical data frame
head(categorical_data)
```

-   Generally, most of the residents having following characteristics:

    -   Low density resident

    -   Paved streets

    -   Gravel alley

    -   Regular sized property

    -   Near Flat level

    -   Available with electric power, natural gas, steam supply, water supply, and sewage removal

    -   Inside lot

    -   Gentle slope of property

    -   Single-family Detached

    -   Single storey

    -   Gable roof

    -   Standard (Composite) Shingle roof

    -   Good quality Vinyl Siding exteriors

    -   Slight damped basement

    -   No walkout or garden level walls

    -   Gas forced warm air furnace

    -   Excellent heating quality

    -   Has Air-conditioning

    -   Standard Circuit Breakers & Romex electrical system

    -   Typical kitchen quality

    -   Typical home functionality

    -   Masonry Fireplace in main level

    -   Typical unfinished garage attached to home

    -   Paved driveway

    -   Good pool quality

    -   Minimum Wood/Wire fence

    -   Warranty Deed -- Conventional Sales

    -   Normal Sales

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Print bar plots for categorical data for the first 20 attributes
plots_first20attribute <- lapply(colnames(categorical_data)[1:20], function(col) {
  ggplot(categorical_data, aes(x = !!sym(col))) +
    geom_bar(fill = "steelblue") +
    xlab(col) +
    ylab("Count") +
    ggtitle(paste0("Distribution of", col)) +
    scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
})

grid.arrange(grobs = plots_first20attribute, ncol = 5)

```

```{r, fig.width=10, fig.height=10, fig.fullwidth=TRUE}
# Print bar plots for categorical data for the remaining 17 attributes
plots_restattribute <- lapply(colnames(categorical_data)[21:37], function(col) {
  ggplot(categorical_data, aes(x = !!sym(col))) +
    geom_bar(fill = "steelblue") +
    xlab(col) +
    ylab("Count") +
    ggtitle(paste0("Distribution of", col)) +
    scale_x_discrete(guide = guide_axis(check.overlap = TRUE))
})

grid.arrange(grobs = plots_restattribute, ncol = 5)
```

### Clustering

#### PCA + K-Means Clustering

-   The code executes PCA (Principal Component Analysis) to reduce the data's dimensionality and extract the principal components.

-   Elbow method used to determine the best number of clusters. Result: 3.

-   K-means clustering is performed on the principal components, assigning each observation to a specific cluster.

-   The cluster assignments are appended to the principal components data, and a scatter plot is generated to visualize the resulting clusters.

```{r}
# Perform PCA on the data matrix
pca_result <- prcomp(X)

# Extract the principal components
principal_components <- as.data.frame(pca_result$x)

# Determine the optimal number of clusters using the elbow method
fviz_nbclust(principal_components, kmeans, method = "wss")

k <- 3
# Perform K-means clustering on the principal components
kmeans_result <- kmeans(principal_components, centers = k)
cluster_assignments <- kmeans_result$cluster
# print(cluster_assignments)

# Add the cluster assignments to the principal components data
principal_components$cluster <- as.factor(cluster_assignments)

ggplot(principal_components, aes(PC1, PC2, color = cluster)) +
  geom_point() +
  labs(x = "Principal Component 1", y = "Principal Component 2") +
  scale_color_discrete(name = "Cluster") +
 theme_minimal()
```

#### t-SNE + Hierarchical Clustering

-   t-SNE (t-Distributed Stochastic Neighbor Embedding) is performed for dimensionality reduction and visualization of the data.

Function:

-   Calculates the t-SNE coordinates

-   Creates a dendrogram through hierarchical clustering

-   Determines the number of clusters visually, assigns data points to clusters

-   Computes centroids for each cluster

-   Generates scatter plots to visualize the t-SNE coordinates colored by the original variable and cluster assignments

-   Includes a bar plot showing the cluster frequencies and evaluates clustering quality using silhouette analysis.

```{r}
tsne <- Rtsne(X)
tsne_df <- data.frame(tsne)
```

```{r}
dist_mat <- dist(tsne$Y, method = "euclidean")
hclust_avg <- hclust(dist_mat, method = "average")

dend <- as.dendrogram(hclust_avg)
plot(dend)
```

```{r}
k = 15
cut_avg <- cutree(hclust_avg, k)
tsne_df$cluster <- cut_avg

getCentroid <- function(points) {
  xy <- numeric(2)
  
  xy[1] = mean(points[, 1])
  xy[2] = mean(points[, 2])

  return(xy)
}

centroids = matrix(0, k, 2)
for (i in unique(cut_avg)) centroids[i, ] <- getCentroid(tsne$Y[cut_avg == i,])
```

#### Silhouette Graph

-   Graph width represents the number of data point; longer length indicates a stronger separation and better-defined clusters.

-   The lighter the color, the higher the house price.

```{r}
ggplot(tsne_df, aes(x=Y.1, y=Y.2, color=y)) +
  geom_point() +
  scale_color_gradientn(colours = heat.colors(10))

ggplot(data.frame(table(tsne_df$cluster)), aes(x=Var1, y=Freq)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Clusters", y = "Sizes")

ggplot(tsne_df, aes(x=Y.1, y=Y.2, color=cluster)) +
  geom_point() +
  scale_color_viridis() +
  scale_fill_viridis(discrete = T) +
  geom_point(data = data.frame(centroids), aes(x=X1, y=X2), color="black", fill="white", shape=21, size=8) +
  geom_text(data = data.frame(centroids), aes(x=X1, y=X2, label=1:k), color="black")

fviz_silhouette(silhouette(cutree(hclust_avg, k = k), dist_mat))

```

### Regression Modelling

#### Baselines

-   Sets up empty lists to store RMSE values for baseline models on the training and testing datasets.

-   Calculate performance metrics (MAE, MAPE, RMSE, MSE, R2) by converting predicted and actual values from transformed to original scale.

-   Focus on data structures and the metrics calculation function.

```{r}
baselines_rmse <- list()

baselines_rmse_test <- list()
actual_metrics_test <- list()

metrics_fusion <- function(y_pred, y) {
  y_pred_inv <- expm1(y_pred)
  y_inv <- expm1(y)

  a <- mae(y_pred_inv, y_inv)
  b <- mape(y_pred_inv, y_inv)
  c <- rmse(y_pred_inv, y_inv)
  d <- mse(y_pred_inv, y_inv)
  e <- R2(y_pred_inv, y_inv)

  return(c("mae" = a, "mape" = b, "rmse" = c, "mse" = d, "r2" = e))
}
```

##### Linear Regression

-   Evaluates the performance of a linear regression model for a regression problem.

-   Cross-validation used to train the model on the training and validation data.

-   Resulting model is applied to make predictions on the validation dataset.

-   Root mean squared error (RMSE) is calculated as a measure of prediction accuracy for the model on the validation data.

-   Trained linear regression model used to make predictions on the test dataset.

-   RMSE is calculated by comparing the predicted values with the actual test values.

-   Stores additional performance metrics (MAE, MAPE, RMSE, MSE, R2) for the linear regression model on the test data.

```{r, warning=FALSE}
linreg_tc <- trainControl(method = "cv", number = 5)
linreg_cv <- caret::train(
  SalePrice ~ .,
  data = cbind(X_train_val, SalePrice = y_train_val),
  trControl = linreg_tc,
  method = "lm"
)

# Validation predictions and metrics
score_val <- linreg_cv$results$RMSE
baselines_rmse$linear_regression <- score_val

# Test predictions and metrics
linreg <- lm(SalePrice ~ ., data = cbind(X_train_val, SalePrice = y_train_val))

y_pred_test <- predict(linreg, newdata = X_test)
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$linear_regression <- score_test
actual_metrics_test$linear_regression <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

##### Lasso Regression

-   Lasso Regression applied to a regression problem.

-   Model trained by using cross-validation and selects the optimal regularization parameter.

-   Root mean squared error (RMSE) calculated for the validation set.

-   For the test set, target variable predicted by using the trained model, calculates the RMSE

-   Actual performance metrics (MAE, MAPE, RMSE, MSE, R2) computed for the Lasso Regression model on the test set

-   Displays the validation RMSE and test RMSE.

```{r}
lasso <- cv.glmnet(x = as.matrix(X_train_val), y = y_train_val, alpha = 1)

# Validation predictions and metrics
score_val <- mean(sqrt(lasso$cvm))
baselines_rmse$lasso <- score_val

# Test predictions and metrics
y_pred_test <- predict(lasso, newx = as.matrix(X_test))
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$lasso <- score_test
actual_metrics_test$lasso <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

##### Ridge Regression

-   Ridge Regression applied to a regression problem.

-   Model trained by using cross-validation and selects the optimal regularization parameter.

-   Root mean squared error (RMSE) calculated for the validation set.

-   For the test set, it predicts the target variable using the trained model, calculates the RMSE.

-   Actual performance metrics (MAE, MAPE, RMSE, MSE, R2) computed for the Ridge Regression model on the test set.

-   Validation RMSE and test RMSE dsiplayed.

```{r}
ridge <- cv.glmnet(x = as.matrix(X_train_val), y = y_train_val, alpha = 0)

# Validation predictions and metrics
score_val <- mean(sqrt(ridge$cvm))
baselines_rmse$ridge <- score_val

# Test predictions and metrics
y_pred_test <- predict(ridge, newx = as.matrix(X_test))
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$ridge <- score_test
actual_metrics_test$ridge <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

##### Elastic Net

-   Elastic Net Regression applied to a regression problem.

-   Cross-validation peformed to select the optimal alpha parameter, which controls the balance between L1 (Lasso) and L2 (Ridge) regularization.

-   The root mean squared error (RMSE) calculated for the validation set.

-   For the test set, it predicts the target variable using the trained Elastic Net model with the selected alpha value, calculates the RMSE.

-   Actual performance metrics (MAE, MAPE, RMSE, MSE, R2) computed for the Elastic Net model on the test set.

-   Selected alpha, validation RMSE, and test RMSE.

```{r}
results <- data.frame()

for (i in 0:20) {
  elasticnet <- cv.glmnet(x = as.matrix(X_train_val), y = y_train_val, alpha = i/20)

  row <- data.frame(alpha = i/20, rmse_val = mean(sqrt(elasticnet$cvm)))
  results <- rbind(results, row)
}

best_alpha <- results$alpha[which.min(results$rmse_val)]

# Validation predictions and metrics
score_val <- min(results$rmse_val)
baselines_rmse$elasticnet <- score_val

# Test predictions and metrics
elasticnet <- cv.glmnet(x = as.matrix(X_train_val), y = y_train_val, alpha = best_alpha)

y_pred_test <- predict(elasticnet, newx = as.matrix(X_test))
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$elasticnet <- score_test
actual_metrics_test$elasticnet <- metrics_fusion(y_pred_test, y_test)

# Display scores
best_alpha
score_val
score_test
```

##### K-Nearest Neighbors Regression

-   K-Nearest Neighbors Regression applied to the data

-   Calculates the RMSE for validation and test sets, and stores the results.

-   Additional performance metrics computed and displays the RMSE values.

```{r}
knn_tc <- trainControl(method = "cv", number = 5)
knn_cv <- caret::train(
  SalePrice ~ .,
  data = cbind(X_train_val, SalePrice = y_train_val),
  trControl = knn_tc,
  method = "knn"
)

# Validation predictions and metrics
score_val <- mean(knn_cv$results$RMSE)
baselines_rmse$knn <- score_val

# Test predictions and metrics
knn <- knnreg(x = X_train_val, y = y_train_val)

y_pred_test <- predict(knn, newdata = X_test)
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$knn <- score_test
actual_metrics_test$knn <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

##### Support Vector Regression

-   Support Vector Regression (SVR) applied to the data

-   Calculates the RMSE for validation and test sets, and stores the results.

-   Additional performance metrics computed and displays the RMSE values.

```{r}
svr_tc <- trainControl(method = "cv", number = 5)
svr_cv <- caret::train(
  SalePrice ~ .,
  data = cbind(X_train_val, SalePrice = y_train_val),
  trControl = svr_tc,
  method = "svmLinear2",
  scale = F
)

# Validation predictions and metrics
score_val <- mean(svr_cv$results$RMSE)
baselines_rmse$svr <- score_val

# Test predictions and metrics
svr <- e1071::svm(SalePrice ~ ., data = cbind(X_train_val, SalePrice = y_train_val), scale = F)

y_pred_test <- predict(svr, newdata = X_test)
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$svr <- score_test
actual_metrics_test$svr <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

##### Decision Tree

-   Decision Tree regression applied to the data.

-   Calculates the RMSE for validation and test sets, and stores the results.

-   Additional performance metrics computed and displays the RMSE values.

```{r, warning=FALSE}
dt_tc <- trainControl(method = "cv", number = 5)
dt_cv <- caret::train(
  SalePrice ~ .,
  data = cbind(X_train_val, SalePrice = y_train_val),
  trControl = dt_tc,
  method = "rpart"
)

# Validation predictions and metrics
score_val <- mean(dt_cv$results$RMSE, na.rm = TRUE)
baselines_rmse$decision_tree <- score_val

# Test predictions and metrics
dt <- caret::train(x = X_train_val, y = y_train_val, method = "rpart")

y_pred_test <- predict(dt, newdata = X_test)
score_test <- rmse(y_pred_test, y_test)
baselines_rmse_test$decision_tree <- score_test
actual_metrics_test$decision_tree <- metrics_fusion(y_pred_test, y_test)

# Display scores
score_val
score_test
```

#### Ensemble Learning

-   Empty lists initialized that will be used to store the RMSE values and actual performance metrics for ensemble learning models.

-   Empty lists will be populated with values in subsequent steps of the code.

```{r}
ensemble_rmse <- list()
ensemble_actual_metrics <- list()

ensemble_rmse_test <- list()
ensemble_actual_metrics_test <- list()
```

##### Random Forest

-   Random forest algorithm implemented for regression tasks using ensemble learning.

-   Random forest model trained on the training and validation data.

-   Its performance is evaluated using root mean squared error (RMSE) on the validation and test datasets.

-   Calculate feature importance using the random forest model and creates a bar plot to visualize the top 30 important features.

```{r}
rf_tc <- trainControl(method = "cv", number = 5)
rf_cv <- caret::train(
  SalePrice ~ .,
  data = cbind(X_train_val, SalePrice = y_train_val),
  trControl = rf_tc,
  method = "rf"
)

# Validation predictions and metrics
score_val <- mean(rf_cv$results$RMSE)
ensemble_rmse$random_forest <- score_val

# Test predictions and metrics
rf <- randomForest(x = X_train_val, y = y_train_val, proximity = T)

y_pred_test_rf <- predict(rf, newdata = X_test)
score_test <- rmse(y_pred_test_rf, y_test)
ensemble_rmse_test$random_forest <- score_test
ensemble_actual_metrics_test$random_forest <- metrics_fusion(y_pred_test_rf, y_test)

y_pred_train_rf <- predict(rf, newdata = X_train_val)

# Display scores
score_val
score_test
```

```{r}
rf_df <- data.frame(rf$importance) %>%
  mutate(Feature = rownames(rf$importance)) %>%
  arrange(desc(IncNodePurity)) %>%
  head(30)

rf_df
ggplot(data = rf_df, aes(x = reorder(Feature, IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Features", y = "Importance")
```

##### Gradient-Boosted Trees

-   Gradient-Boosted Trees (GBT) using XGBoost, LightGBM, and CatBoost libraries implemented for regression tasks.

-   The models are trained on the training and validation data, and their performance is evaluated using RMSE on the validation and test datasets.

-   The best hyperparameters are determined through cross-validation, and the models are then used to make predictions on the test data.

-   The RMSE scores and other evaluation metrics are recorded for each model.

-   The feature importance of the GBT model is visualized using a bar chart.

-   Code snippet showcases the application of ensemble learning techniques for regression tasks using boosting algorithms.

```{r}
dtrain_val <- xgb.DMatrix(data = as.matrix(X_train_val), label = y_train_val)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

xgb_params = list(
  eta = 0.01,
  gamma = 0.0468,
  max_depth = 6,
  min_child_weight = 1.41,
  subsample = 0.769,
  colsample_bytree = 0.283
)

xgb_cv <- xgb.cv(
  params = xgb_params,
  data = dtrain_val,
  nround = 10000,
  nfold = 5,
  prediction = F,
  showsd = T,
  metrics = "rmse",
  verbose = 1,
  print_every_n = 500,
  early_stopping_rounds = 25
)

# Validation predictions and metrics
score_val <- xgb_cv$evaluation_log$test_rmse_mean %>% min
ensemble_rmse$xgboost <- score_val

# Test predictions and metrics
xgb <- xgboost(
  params = xgb_params,
  data = dtrain_val,
  nround = 10000,
  eval_metric = "rmse",
  verbose = 1,
  print_every_n = 500,
  early_stopping_rounds = 25
)

y_pred_test_xgb <- predict(xgb, newdata = dtest)
score_test <- rmse(y_pred_test_xgb, y_test)
ensemble_rmse_test$xgboost <- score_test
ensemble_actual_metrics_test$xgboost <- metrics_fusion(y_pred_test_xgb, y_test)

y_pred_train_xgb <- predict(xgb, newdata = dtrain_val)

# Display scores
score_val
score_test
```

```{r}
xgb_df <- xgb.importance(model = xgb) %>% head(30)

xgb_df
ggplot(data = xgb_df, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Features", y = "Importance")
```

```{r}
objective_fn <- makeSingleObjectiveFunction(
  fn = function(x) {
    params = list(
      booster = "gbtree",
      eta = x["eta"],
      gamma = x["gamma"],
      max_depth = x["max_depth"],
      min_child_weight = x["min_child_weight"],
      subsample = x["subsample"],
      colsample_bytree = x["colsample_bytree"],
      max_delta_step = x["max_delta_step"]
    )

    cv <- xgb.cv(
      params = params,
      data = dtrain_val,
      nround = 10000,
      nfold = 5,
      prediction = F,
      showsd = T,
      metrics = "rmse",
      verbose = 1,
      print_every_n = 500,
      early_stopping_rounds = 25
    )

    cv$evaluation_log$test_rmse_mean %>% min
  },
  par.set = makeParamSet(
    makeNumericParam("eta", lower = 0.005, upper = 0.01),
    makeNumericParam("gamma", lower = 0.01, upper = 5),
    makeIntegerParam("max_depth", lower = 2, upper = 10),
    makeIntegerParam("min_child_weight", lower = 1, upper = 2000),
    makeNumericParam("subsample", lower = 0.20,  upper = 0.8),
    makeNumericParam("colsample_bytree", lower = 0.20, upper = 0.8),
    makeNumericParam("max_delta_step", lower = 0, upper = 5)
  ),
  minimize = TRUE
)

# Define experiments and an objective function
design <- generateDesign(n = 1000, par.set = getParamSet(objective_fn), fun = lhs::randomLHS)
control <- makeMBOControl() %>% setMBOControlTermination(., iters = 10)

# run <- mbo(
#  fun = objective_fn,
#  design = design,
#  learner = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2", control = list(trace = FALSE)),
#  control = control,
#  show.info = TRUE
# )

# Best parameters
# run$x
```

```{r}
lgb_train_val <- lgb.Dataset(data = as.matrix(X_train_val), label = y_train_val)
lgb_test <- lgb.Dataset(data = as.matrix(X_test), label = y_test)

params <- list(
  objective = "regression",
  metric = "rmse",
  boosting_type = "gbdt",
  num_boost_round = 100,
  num_leaves = 15,
  learning_rate = 0.1,
  feature_fraction = 0.9,
  bagging_fraction = 0.8,
  bagging_freq = 5,
  force_row_wise = T
)

lgb_cv <- lgb.cv(
  params = params,
  data = lgb_train_val,
  early_stopping_rounds = 25,
  verbose = 0
)

# Validation predictions and metrics
score_val <- min(unlist(lgb_cv$record_evals$valid$rmse$eval))
ensemble_rmse$lightgbm <- score_val

# Test predictions and metrics
lgb <- lgb.train(
  params = params,
  data = lgb_train_val,
  verbose = 0
)

y_pred_test_lgb <- predict(lgb, data = as.matrix(X_test))
score_test <- rmse(y_pred_test_lgb, y_test)
ensemble_rmse_test$lightgbm <- score_test
ensemble_actual_metrics_test$lightgbm <- metrics_fusion(y_pred_test_lgb, y_test)

y_pred_train_lgb <- predict(lgb, data = as.matrix(X_train_val))

# Display scores
score_val
score_test
```

```{r}
lgb_df <- lgb.importance(model = lgb) %>% head(30)

lgb_df
ggplot(data = xgb_df, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Features", y = "Importance")
```

```{r}
train_val_pool <- catboost.load_pool(data = X_train_val, label = y_train_val)
test_pool <- catboost.load_pool(data = X_test, label = y_test)

params <- list(
  loss_function = "RMSE",
  iterations = 10000,
  learning_rate = 0.01,
  metric_period = 1000
)

catb_cv <- catboost.cv(
  train_val_pool,
  params = params,
  fold_count = 5,
  early_stopping_rounds = 25
)

# Validation predictions and metrics
score_val <- min(catb_cv$test.RMSE.mean)
ensemble_rmse$catboost <- score_val

# Test predictions and metrics
catb <- catboost.train(
  params = params,
  learn_pool = train_val_pool
)

y_pred_test_catboost <- catboost.predict(catb, test_pool)
score_test <- rmse(y_pred_test_catboost, y_test)
ensemble_rmse_test$catboost <- score_test
ensemble_actual_metrics_test$catboost <- metrics_fusion(y_pred_test_catboost, y_test)

y_pred_train_catboost <- catboost.predict(catb, train_val_pool)

# Display scores
score_val
score_test
```

```{r}
catb_df <- data.frame(catboost.get_feature_importance(catb))
catb_df <- catb_df %>%
  mutate(Feature = rownames(catb_df)) %>%
  rename(Importance = catboost.get_feature_importance.catb.) %>%
  arrange(desc(Importance)) %>%
  head(30)

catb_df
ggplot(data = catb_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Features", y = "Importance")
```

##### Stacking

-   Stacking is performed by combining the predictions from different models (Random Forest, XGBoost, LightGBM, CatBoost) into a new dataset.

-   Linear regression model is trained on the stacked dataset to learn the relationship between ensemble predictions and the actual target values.

-   Model's predictions on the test dataset are evaluated using root mean squared error (RMSE).

-   Metrics fusion function is applied to assess the performance of the stacking ensemble.

```{r}
stacked_data <- data.frame(y = y_train_val, prediction_rf = y_pred_train_rf, prediction_xgb = y_pred_train_xgb, prediction_lgb = y_pred_train_lgb, prediction_catboost = y_pred_train_catboost)

stacked_data_test <- data.frame(y = y_test, prediction_rf = y_pred_test_rf, prediction_xgb = y_pred_test_xgb, prediction_lgb = y_pred_test_lgb, prediction_catboost = y_pred_test_catboost)

model_meta <- caret::train(y ~ ., data = stacked_data, method = "lm")
predictions_meta <- predict(model_meta, newdata = stacked_data_test)

ensemble_rmse_test$stacking <- rmse(predictions_meta, y_test)
ensemble_actual_metrics_test$stacking <- metrics_fusion(predictions_meta, y_test)
```

#### Comparison

-   Compares the performance of different models and ensembles using the root mean squared error (RMSE) metric.

-   Bar charts created to display the RMSE values, with the x-axis representing the models and the y-axis representing the RMSE.

-   The first plot focuses on the evaluation using the validation set.

-   The second plot focuses on the evaluation using the test set.

-   Purpose is to compare the models' performance and determine which ones have the lowest RMSE, indicating better predictive accuracy.

```{r}
# Plot validation loss
rmse_all <- c(unlist(baselines_rmse), unlist(ensemble_rmse))
rmse_all_names <- c(names(baselines_rmse), names(ensemble_rmse))

df <- data.frame(models = rmse_all_names, rmse = rmse_all)
df %>% arrange(rmse)

ggplot(df, aes(x = factor(models, level=rmse_all_names), y = rmse)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  xlab("Models") +
  ylab("RMSE") +
  ylim(0, 0.3) +
  ggtitle("Validation Loss") +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme_minimal()
```

```{r}
# Plot test loss
rmse_all_test <- c(unlist(baselines_rmse_test), unlist(ensemble_rmse_test))
rmse_all_test_names <- c(names(baselines_rmse_test), names(ensemble_rmse_test))

df <- data.frame(models = rmse_all_test_names, rmse = rmse_all_test)
df %>% arrange(rmse)

ggplot(df, aes(x = factor(models, level=rmse_all_test_names), y = rmse)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  xlab("Models") +
  ylab("RMSE") +
  ylim(0, 0.3) +
  ggtitle("Test Loss") +
  scale_x_discrete(guide = guide_axis(angle = 30)) +
  theme_minimal()
```

```{r}
data.frame(t(data.frame(actual_metrics_test))) %>% arrange(desc(r2))
data.frame(t(data.frame(ensemble_actual_metrics_test))) %>% arrange(desc(r2))
```

### Association Rule Mining on Housing Characteristics

-   Code snippet performs association rule mining on housing characteristics using the Apriori algorithm.

-   Converts categorical data into transactions and applies the Apriori algorithm to find frequent itemsets.

-   Generate association rules using given support and confidence thresholds.

-   The code includes a dictionary to map column names and values to their corresponding descriptions for better rule interpretation.

-   Top ten association rules with their corresponding explanations displayed, including the antecedent (IF) and consequent (THEN) parts along with their descriptions and confidence percentages.

```{r, warning=FALSE}
transactions <- transactions(categorical_data)
summary(transactions)
```

```{r}
inspect(head(transactions, n = 1))
```

```{r}
rules <- apriori(transactions, parameter = list(support = 0.95, confidence = 0.95))
```

```{r}
summary(rules)
```

```{r}
con <- file("data_description.txt", open = "r")

column_dictionary <- list()
value_dictionary <- list()

repeat {
  line <- readLines(con, n = 1)

  if (length(line) == 0) {
    break
  }

  first_character <- substr(line, 1, 1)

  if (first_character == "") {
    next
  }

  if (first_character != " ") {
    column_name <- sub(":.*", "", line)
    column_description <- trimws(sub(".*:", "", line))

    column_dictionary[[column_name]] <- column_description
    value_dictionary[[column_name]] <- list()
  } else {
    pairs <- unlist(strsplit(line, "\t"))
    key <- trimws(pairs[1])
    value <- trimws(pairs[2])

    value_dictionary[[column_name]][[key]] <- value
  }
}

close(con)
```

```{r}
rules_top_ten_df <- data.frame(
  lhs = labels(lhs(rules)),
  rhs = labels(rhs(rules)),
  rules@quality
) %>% arrange(desc(lift)) %>% head(n = 20)
```

```{r}
for (i in 1:nrow(rules_top_ten_df)) {
  row <- rules_top_ten_df[i, ]

  explanation <- ""
  lhs <- unlist(strsplit(gsub('^.|.$', '', row["lhs"]), ","))

  for (i in 1:length(lhs)) {
    pair <- unlist(strsplit(lhs[i], "="))
    key <- pair[1]
    value <- pair[2]

    key_t <- column_dictionary[[key]]
    value_t <- value_dictionary[[key]][[value]]

    if (i == 1) {
      explanation <- paste("IF", key_t, "=", value_t)
    } else {
      explanation <- paste(explanation, "AND", key_t, "=", value_t)
    }
  }

  rhs <- unlist(strsplit(gsub('^.|.$', '', row["rhs"]), "="))
  key <- rhs[1]
  value <- rhs[2]

  key_t <- column_dictionary[[key]]
  value_t <- value_dictionary[[key]][[value]]

  confidence_pct <- format(round(row["confidence"] * 100, 2), 2)

  explanation <- paste(explanation, "THEN", key_t, "=", value_t, "(Confidence:", paste0(confidence_pct, "%)"))
  print(explanation)
  cat("\n")
}
```

### Conclusion

-   Several regression models were trained and evaluated on a dataset.

-   The linear regression model, trained using cross-validation, achieved good results with an RMSE score of approximately 0.1408 on the validation dataset and 0.1351 on the test dataset.

-   Lasso regression, which incorporates feature selection, had an average RMSE score of around 0.1654 on validation and 0.1361 on the test dataset.

-   Ridge regression, focusing on reducing multicollinearity, had an average RMSE score of approximately 0.2432 on validation and 0.1434 on the test dataset.

-   Elastic Net regression, combining Lasso and Ridge regularization, performed well with an RMSE score of around 0.1639 on validation and 0.1356 on the test dataset.

-   K-Nearest Neighbors regression achieved moderate performance with an average RMSE score of approximately 0.1832 on validation and 0.1881 on the test dataset.

-   The SVR model, based on support vector regression, consistently performed well with an average RMSE score of 0.1419 on validation and 0.1327 on the test dataset.

-   The decision tree model had an average RMSE score of 0.2999 on validation and 0.2723 on the test dataset.

-   In comparison, the random forest model, an ensemble of decision trees, outperformed the decision tree model with an average RMSE score of 0.1714 on validation and 0.1357 on the test dataset.

-   The random forest model highlighted influential features such as "OverallQual," "GrLivArea," and "YearBuilt" in predicting sale prices.

-   Among the gradient-boosted tree models, XGBoost and LightGBM had similar performance.

-   XGBoost achieved a validation RMSE of 0.1245088 and a test RMSE of 0.1244002, while LightGBM had a validation RMSE of 0.1302412 and a test RMSE of 0.1376078.

-   CatBoost had slightly higher errors with a validation RMSE of 0.1918585 and a test RMSE of 0.1245417.

-   Key factors influencing house prices included OverallQual, GrLivArea, and GarageArea.

-   Overall, the linear regression, Lasso regression, Elastic Net regression, SVR, random forest, XGBoost, and LightGBM models showed promising performance in predicting sale prices.

-   These models can provide valuable insights for decision-making in the real estate market.

-   In the analysis, comparison the performance of baseline models and ensemble models for the given dataset will be occur.

-   The evaluation metric used was Root Mean Squared Error (RMSE), which measures the average prediction error.

-   RMSE provides a measure of the average prediction error in the same units as the target variable.

-   It is useful for comparing different models or evaluating the performance of a single model.

-   Lower RMSE values indicate better predictive accuracy, as they imply smaller average errors between predicted and actual values.

-   Higher RMSE values indicate worse predictive accuracy, as they imply bigger average errors between predicted and actual values.

#### Baseline Models (Training Set):

-   Six models had been used which are Linear regression, Lasso, Ridge, Elasticnet, Knn, Svr and Decision tree.

-   Among the baseline models, linear regression models had the lowest Root Mean Squared Error (RMSE), followed closely by Svr.

-   Knn, Elasticnet, Lasso and Ridge performed relatively well

-   However, Decision Tree had the highest RMSE, indicating poorer predictive performance.

#### Ensemble Models (Training Set):

-   The ensemble models, including Random Forest, XGBoost, LightGBM, and CatBoost, showed improved performance compared to individual baseline models.

-   LightGBM achieved the lowest RMSE on the test set, closely followed by XGBoost.

-   The highest RMSE is catboost and followed by Random Forest, indicating poorer predictive performance.

#### Baseline Models (Test Set):

-   Based on the provided plot and data for the baseline models' RMSE on the test set, Decision tree had the highest Root Mean Squared Error (RMSE) while Svr had the lowest Root Mean Squared Error (RMSE).

-   Decision tree showed the worst predictive performance, closely followed by Knn and Ridge.

-   SVR showed the best predictive performance, closely followed by Linear Regression, ElasticNet and Lasso.

#### Ensemble Models (Test Set):

-   Based on the provided plot and data for the ensemble models' RMSE on the test set, XGBoost and CatBoost achieved the lowest Root Mean Squared Error (RMSE) value.

-   They were closely followed by the stacking model.

-   These three models exhibited the best predictive performance among the ensemble models.

-   LightGBM had the highest RMSE but it also performed relatively well.

-   Random Forest had a slightly higher RMSE compared to the top-performing ensemble models but still demonstrated a good predictive performance.

#### Comparison

-   In baseline models, Svr, Elasticnet, Linear_regression and Lasso showed the best predictive performance.

-   XGBoost, CatBoost, and the stacking model showed the best predictive performance among the ensemble models based on the RMSE metric.

-   These models have the potential to provide more accurate predictions for the target variable on the test set.
